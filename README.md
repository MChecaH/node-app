# Auto-instrumenting your Node.js deployments

This walkthrough will aim to provide a way to reliably install and deploy an [OpenTelemetry Collector](https://opentelemetry.io/docs/collector/) on an existing Node.js application without the need of modifying its source code. Traces scraped by the Collector may be exported to different backends of the cluster-admin's choice.

These backends are, but not limited to:

- [**Jaeger**](https://www.jaegertracing.io/) - [Deployment guide](#jaeger)
- [**Prometheus**](https://prometheus.io/) - [Deployment guide](#prometheus)
- [**Kibana**](https://www.elastic.co/kibana)

# OpenTelemetry

[OpenTelemetry](https://opentelemetry.io/) is an Observability(Add note) framework. It is designed to create, manage, and distribute telemetry data such as traces(Add note), metrics(Add note), and logs(Add note). It is **not** an observability backend. Its main goal is to distribute signals and instrument applications in your application or system.

## OpenTelemetry Collector

The OpenTelemetry Collector offers a vendor-agnostic agent that receives, processes, and exports telemetry data from multiple, different sources to their respective backends. It ensures having to run the least amount of agents and/or collectors possible from their respective vendors by funneling all data into one same collector.

![](https://opentelemetry.io/docs/collector/img/otel-collector.svg)

## Deployment

Assuming [Helm](https://helm.sh/docs/intro/install/) is already installed in your current deployment, download and install the `cert-manager` dependencies:

```bash
$ helm repo add jetstack https://charts.jetstack.io --force-update
```

Next, install it into your cluster:

```bash
$ helm install \
    cert-manager jetstack/cert-manager \
    --namespace cert-manager \
    --create-namespace \
    --version v1.15.3 \
    --set crds.enabled=true
```

Optionally, to verify whether the installation was successful, please refer to the [official `cert-manager` documentation](https://cert-manager.io/docs/installation/kubectl/#verify).

Next, we'll be able to deploy the Collector into an existing cluster with the following command:

```bash
$ kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/latest/download/opentelemetry-operator.yaml
```

Once the CRD is applied in the cluster, you'll be able to configure the collector and its instrumentation resources to the needs of your application.

## Configuration

### OpenTelemetryCollector

This resource defines how the Collector should behave. It does this by offering multiple configurations for its [Receivers](#receivers), [Processors](#processors), [Exporters](#exporters), and [Pipelines](#pipelines). Processors are optional, as a Pipeline can be comprised of just a Receiver and an Exporter. By default, the `-collector` suffix will be added to the services generated by this deployment.

```yaml
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: demo
spec:
  config: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

    processors:
      memory_limiter:
        check_interval: 1s
        limit_percentage: 75
        spike_limit_percentage: 15
      batch:
        send_batch_size: 10000
        timeout: 10s

    exporters:
        otlp/jaeger:
        endpoint: simplest-collector:4317
        tls:
          insecure: true
      prometheus/prom:
        endpoint: '0.0.0.0:9090'

    service:
      telemetry:
        metrics:
          address: 0.0.0.0:8888
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [otlp/jaeger]
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [prometheus/prom]
```

### Instrumentation

An Instrumentation resource defines how the auto-instrumentation of an application should be carried out. By default, an empty instrumentation resource will instrument **ALL** possible telemetry data for most [supported languages](https://opentelemetry.io/docs/zero-code/).

```yaml
apiVersion: opentelemetry.io/v1alpha1
kind: Instrumentation
metadata:
  name: demo-instrumentation
  namespace: node-app
spec:
  nodejs:
    image: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-nodejs:latest
  exporter:
    endpoint: http://demo-collector.default.svc.cluster.local:4317
  propagators:
    - tracecontext
    - baggage
  sampler:
    type: parentbased_traceidratio
    argument: "1"
```

### Choosing which deployments to auto-instrument

For an application to be auto-instrumented, the operator needs to inject its agents into its containers. To do this, we must modify the main `Deployment` of the application as add the injection labels for our language of choice under `spec.template.metadata.annotations`.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: node-mainapp
  namespace: node-app
  labels:
    app: nodeapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nodeapp
  template:
    metadata:
      labels:
        app: nodeapp
      annotations: # The following annotations must be added
        sidecar.opentelemetry.io/inject: "true"
        instrumentation.opentelemetry.io/inject-nodejs: "true"
    spec:
      containers:
        - name: node-app
          image: mchecah/node-app:latest
          ports:
            - containerPort: 3000
        - name: node-app-login
          image: mchecah/node-app-login:latest
          ports:
            - containerPort: 3001
```

# Jaeger

[Jaeger](https://www.jaegertracing.io/) is an open-source distributed tracing platform. It allows to monitor and troubleshoot distributed workflows, identify performance bottlenecks, analyze service dependencies, and easily spot issues in a distributed network from their very root.

## Deployment

To install the operator, run:

```bash
$ kubectl create namespace observability
$ kubectl create -f https://github.com/jaegertracing/jaeger-operator/releases/download/v1.60.0/jaeger-operator.yaml -n observability
```

By default, the installation is done in cluster wide mode. To only watch specific namespaces, the `ClusterRole` and `ClusterRoleBinding` of the manifest must be changed to `Role` and `RoleBinding`. The `WATCH_NAMESPACE` environment variable must also be set on the jaeger operator Deployment.

A [Production installation](https://www.jaegertracing.io/docs/1.60/operator/#production-strategy) can be found in the official docs. For testing purposes, the [All-in-one](https://www.jaegertracing.io/docs/1.60/operator/#quick-start---deploying-the-allinone-image) image should be used, which can be deployed with the following resource:

```yaml
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: simplest
```

And subsequently applied:

```bash
$ kubectl apply -f simplest.yaml
```

## Configuration

To configure the different resources, please refer to the [official documentation](https://www.jaegertracing.io/docs/1.60/operator/#configuring-the-custom-resource). This test uses all of the default settings from the All-in-one installation.

# Information and documentation

- [OpenTelemetry k8s docs](https://opentelemetry.io/docs/kubernetes/)
- [OpenTelemetry Operator API](https://github.com/open-telemetry/opentelemetry-operator/blob/main/docs/api.md)